{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eeca289-1d65-455f-a6d1-4c15c1125f90",
   "metadata": {},
   "source": [
    "##### 1. Explain the properties of the F-distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48121d74-d66e-45f5-abbc-246d051e9e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F-distribution is a continuous probability distribution that arises frequently in the context of variance analysis, particularly in the analysis of variance (ANOVA) and regression analysis. Here are its key properties:\n",
    "# Properties of the F-Distribution\n",
    "\n",
    "# Non-Negative Values: The F-distribution only takes on positive values. This is because it is defined as the ratio of two chi-squared distributions, which are always non-negative.\n",
    "\n",
    "# Right-Skewed: The F-distribution is right-skewed, meaning it has a long tail on the right side. The degree of skewness decreases as the degrees of freedom increase.\n",
    "\n",
    "# Degrees of Freedom: The shape of the F-distribution is determined by two parameters, the degrees of freedom of the numerator ((d_1)) and the degrees of freedom of the denominator ((d_2)). These parameters affect the height and spread of the distribution.\n",
    "\n",
    "# Mean and Variance:\n",
    "\n",
    "# Mean: The mean of the F-distribution is given by:Mean=d2​−2d2​​ford2​>2\n",
    "\n",
    "# Variance: The variance of the F-distribution is:Variance=d1​(d2​−2)2(d2​−4)2d22​(d1​+d2​−2)​ford2​>4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Asymptotic Behavior: As the degrees of freedom (d_1) and (d_2) increase, the F-distribution approaches a normal distribution.\n",
    "\n",
    "\n",
    "# Use in Hypothesis Testing: The F-distribution is commonly used in hypothesis testing, particularly in the context of comparing variances. For example, in ANOVA, it is used to determine whether there are significant differences between group means.\n",
    "\n",
    "\n",
    "# Industry Use Case\n",
    "# In quality control, the F-distribution is used to compare the variances of different production processes to ensure consistency and quality. For instance, a manufacturer might use an F-test to compare the variability in the thickness of materials produced by two different machines.\n",
    "# Real-Life Example\n",
    "# Suppose a researcher wants to compare the effectiveness of three different teaching methods on student performance. They collect test scores from students taught using each method and perform an ANOVA test. The F-distribution helps determine whether the observed differences in test scores are statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e16697-805b-4d8d-b5c4-3e45aae49bb6",
   "metadata": {},
   "source": [
    "##### 2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97f4196-ef3b-4db3-9838-be408c655de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The F-distribution is used in several types of statistical tests, primarily those that involve comparing variances or testing the overall significance of models. Here are the main types of tests where the F-distribution is commonly applied:\n",
    "\n",
    "# 1. Analysis of Variance (ANOVA)\n",
    "# Purpose: To compare the means of three or more groups to see if at least one group mean is significantly different from the others.\n",
    "\n",
    "# Why F-Distribution: ANOVA uses the F-distribution to determine whether the between-group variability is significantly greater than the within-group variability. The F-statistic is calculated as the ratio of the mean square between groups to the mean square within groups.\n",
    "\n",
    "# Example: Comparing the effectiveness of different fertilizers on plant growth.\n",
    "\n",
    "# 2. Regression Analysis\n",
    "# Purpose: To test the overall significance of a regression model.\n",
    "\n",
    "# Why F-Distribution: In regression analysis, the F-test is used to compare the fit of the model with and without the predictors. It helps determine if the predictors explain a significant portion of the variance in the dependent variable.\n",
    "\n",
    "# Example: Evaluating whether variables like advertising spend and price affect sales.\n",
    "\n",
    "# 3. Comparing Two Variances (F-Test)\n",
    "# Purpose: To compare the variances of two independent samples to see if they are significantly different.\n",
    "\n",
    "# Why F-Distribution: The F-test for comparing two variances uses the ratio of the two sample variances. The F-distribution is appropriate because it models the distribution of this ratio under the null hypothesis that the variances are equal.\n",
    "\n",
    "# Example: Comparing the variability in test scores between two different teaching methods.\n",
    "\n",
    "# 4. Testing the Equality of Multiple Variances (Levene’s Test)\n",
    "# Purpose: To test the assumption of equal variances (homogeneity of variances) across multiple groups.\n",
    "\n",
    "# Why F-Distribution: Levene’s test uses the F-distribution to determine if the variances are significantly different from each other. This is important for validating assumptions in ANOVA and other parametric tests.\n",
    "\n",
    "# Example: Checking if the variance in customer satisfaction scores is the same across different service centers.\n",
    "\n",
    "# Why the F-Distribution is Appropriate\n",
    "# Ratio of Variances: The F-distribution is specifically designed to handle ratios of variances, making it ideal for tests that compare variability.\n",
    "# Degrees of Freedom: It accounts for the degrees of freedom in both the numerator and the denominator, providing a flexible and accurate model for various sample sizes.\n",
    "# Right-Skewed Nature: Its right-skewed nature aligns well with the distribution of variance ratios, especially when sample sizes are small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b687e7-657c-4a8c-a1c1-944a26f916bd",
   "metadata": {},
   "source": [
    "##### 3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd11e87-8d82-474c-8b89-a8218cda39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To conduct an F-test for comparing the variances of two populations, several key assumptions must be met to ensure the validity of the test results. Here are the primary assumptions:\n",
    "\n",
    "# Key Assumptions for an F-Test\n",
    "# Independence:\n",
    "# The samples must be independent of each other. This means that the selection of one sample should not influence the selection of the other sample.\n",
    "# Example: If you are comparing the variances of test scores from two different classes, the scores from one class should not affect the scores from the other class.\n",
    "\n",
    "# Normality:\n",
    "# The populations from which the samples are drawn should be normally distributed. This assumption is crucial because the F-test is sensitive to deviations from normality.\n",
    "# Example: The distribution of heights in two different groups of people should follow a normal distribution.\n",
    "\n",
    "# Random Sampling:\n",
    "# The samples should be randomly selected from the populations. This ensures that the samples are representative of the populations.\n",
    "# Example: Randomly selecting students from two different schools to compare their test score variances.\n",
    "\n",
    "# Ratio of Variances:\n",
    "# The F-test assumes that the ratio of the variances follows an F-distribution under the null hypothesis. This means that the test statistic, which is the ratio of the sample variances, should follow the F-distribution.\n",
    "# Example: If the variances of two samples are equal, the ratio of their variances should follow the F-distribution.\n",
    "\n",
    "# Why These Assumptions Matter\n",
    "# Independence ensures that the results are not biased by any relationship between the samples.\n",
    "# Normality is important because the F-distribution is derived under the assumption of normality. Deviations from normality can lead to incorrect conclusions.\n",
    "# Random Sampling helps in generalizing the results to the entire population.\n",
    "# Ratio of Variances ensures that the test statistic follows the expected distribution, allowing for accurate hypothesis testing.\n",
    "# Industry Use Case\n",
    "# In manufacturing, an F-test might be used to compare the variability in the thickness of materials produced by two different machines. Ensuring these assumptions are met helps in making reliable decisions about the consistency of the production process.\n",
    "\n",
    "# Real-Life Example\n",
    "# A researcher comparing the variability in blood pressure readings between two different groups of patients (e.g., those on different medications) would need to ensure these assumptions are met to validate the results of the F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f12ea20-febb-4571-953b-bb089a280c7f",
   "metadata": {},
   "source": [
    "##### 4. What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0b1fca-7ae8-4465-a54e-c55efcd4fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose of ANOVA (Analysis of Variance)\n",
    "# ANOVA is a statistical method used to compare the means of three or more groups to determine if there are any statistically significant differences between them. It helps in understanding whether the observed differences among group means are due to actual differences or just random variation.\n",
    "# Key Points:\n",
    "\n",
    "# Hypothesis Testing: ANOVA tests the null hypothesis that all group means are equal against the alternative hypothesis that at least one group mean is different.\n",
    "# F-Statistic: It uses the F-distribution to determine the significance of the results.\n",
    "# Between-Group and Within-Group Variability: ANOVA compares the variability between groups to the variability within groups.\n",
    "\n",
    "# Example: Comparing the average test scores of students from three different teaching methods to see if one method is more effective than the others.\n",
    "# How ANOVA Differs from a t-Test\n",
    "# 1. Number of Groups Compared:\n",
    "\n",
    "# t-Test: Typically used to compare the means of two groups.\n",
    "\n",
    "# Example: Comparing the average heights of men and women.\n",
    "\n",
    "# ANOVA: Used to compare the means of three or more groups.\n",
    "\n",
    "# Example: Comparing the average sales of four different marketing strategies.\n",
    "\n",
    "# 2. Types of t-Tests:\n",
    "\n",
    "# Independent t-Test: Compares means from two independent groups.\n",
    "# Paired t-Test: Compares means from the same group at different times or under different conditions.\n",
    "\n",
    "# 3. Hypothesis Structure:\n",
    "\n",
    "# t-Test: Tests the null hypothesis that the means of two groups are equal.\n",
    "\n",
    "# Formula: t=n1​s12​​+n2​s22​​​Xˉ1​−Xˉ2​​\n",
    "\n",
    "# ANOVA: Tests the null hypothesis that all group means are equal.\n",
    "\n",
    "# Formula: F=Within-group variabilityBetween-group variability​\n",
    "\n",
    "\n",
    "# 4. Flexibility:\n",
    "\n",
    "# t-Test: Limited to comparing two groups.\n",
    "# ANOVA: Can handle multiple groups and can be extended to more complex designs (e.g., factorial ANOVA).\n",
    "\n",
    "# 5. Post-Hoc Tests:\n",
    "\n",
    "# t-Test: Directly compares two means.\n",
    "# ANOVA: If the null hypothesis is rejected, post-hoc tests (e.g., Tukey’s HSD) are used to determine which specific groups differ.\n",
    "\n",
    "# Industry Use Case\n",
    "# In clinical trials, ANOVA is used to compare the effectiveness of multiple treatments. For instance, a pharmaceutical company might use ANOVA to compare the effects of three different drugs on blood pressure.\n",
    "# Real-Life Example\n",
    "# A company wants to compare the productivity of employees working under different management styles. ANOVA can help determine if there are significant differences in productivity across the different management styles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8872754c-46c2-453b-9371-37988af4e341",
   "metadata": {},
   "source": [
    "##### 5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06161e10-97f8-42dc-b57a-c452013f11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When to Use One-Way ANOVA\n",
    "# You should use a one-way ANOVA when you need to compare the means of three or more independent groups to determine if there is a statistically significant difference among them. This is particularly useful when you have one independent variable with multiple levels (groups).\n",
    "\n",
    "# Example: Comparing the average test scores of students from three different teaching methods.\n",
    "\n",
    "# Why Use One-Way ANOVA Instead of Multiple t-Tests\n",
    "# Control of Type I Error Rate:\n",
    "# Type I Error: The probability of incorrectly rejecting a true null hypothesis (false positive).\n",
    "# Multiple t-Tests: Conducting multiple t-tests increases the risk of Type I errors. For example, if you perform three t-tests at a 5% significance level, the overall chance of making at least one Type I error is higher than 5%.\n",
    "# One-Way ANOVA: Controls the overall Type I error rate by testing all group means simultaneously with a single test.\n",
    "# Efficiency:\n",
    "# Multiple t-Tests: Requires more calculations and comparisons, which can be time-consuming and complex.\n",
    "# One-Way ANOVA: Provides a single test statistic (F-statistic) to determine if there are any significant differences among the group means, making it more efficient.\n",
    "# Interpretation:\n",
    "# Multiple t-Tests: Results can be harder to interpret because you have multiple p-values to consider.\n",
    "# One-Way ANOVA: Provides a clear overall test of whether there are any differences among the group means. If the ANOVA is significant, post-hoc tests can be used to identify which specific groups differ.\n",
    "# Assumptions:\n",
    "# Both Tests: Assume normality, independence, and homogeneity of variances.\n",
    "# One-Way ANOVA: These assumptions are easier to manage and check in a single test compared to multiple t-tests.\n",
    "# Industry Use Case\n",
    "# In marketing, a company might want to compare the effectiveness of different advertising campaigns on sales. Using a one-way ANOVA allows them to test all campaigns simultaneously, ensuring a more reliable and interpretable result.\n",
    "\n",
    "# Real-Life Example\n",
    "# A researcher wants to compare the average blood pressure reduction from three different diets. Using a one-way ANOVA helps determine if there is a significant difference in effectiveness among the diets without increasing the risk of Type I errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21cc6ae-f11a-4b5d-8088-5d4c44e9c0f1",
   "metadata": {},
   "source": [
    "##### 6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3223095-d054-41aa-b03a-949726bcd9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partitioning Variance in ANOVA\n",
    "# In ANOVA, the total variance observed in the data is partitioned into two components: between-group variance and within-group variance. This partitioning helps in understanding the sources of variability and is crucial for calculating the F-statistic.\n",
    "# 1. Total Variance (SST)\n",
    "# The total variance (Sum of Squares Total, SST) represents the overall variability in the data. It is the sum of the squared differences between each observation and the overall mean.\n",
    "# Formula:\n",
    "# SST=i=1∑n​(Xi​−Xˉ)2\n",
    "\n",
    "# 2. Between-Group Variance (SSB)\n",
    "# Between-group variance (Sum of Squares Between, SSB) measures the variability due to the differences between the group means. It reflects how much the group means deviate from the overall mean.\n",
    "# Formula:\n",
    "# SSB=j=1∑k​nj​(Xˉj​−Xˉ)2\n",
    "# where ( n_j ) is the number of observations in group ( j ), ( \\bar{X}_j ) is the mean of group ( j ), and ( \\bar{X} ) is the overall mean.\n",
    "\n",
    "# 3. Within-Group Variance (SSW)\n",
    "# Within-group variance (Sum of Squares Within, SSW) measures the variability within each group. It reflects how much the individual observations within each group deviate from their respective group means.\n",
    "# Formula:\n",
    "# SSW=j=1∑k​i=1∑nj​​(Xij​−Xˉj​)2\n",
    "# where ( X_{ij} ) is the ( i )-th observation in group ( j ).\n",
    "# Calculation of the F-Statistic\n",
    "# The F-statistic is calculated by comparing the between-group variance to the within-group variance. It helps determine whether the observed differences among group means are statistically significant.\n",
    "# Steps to Calculate the F-Statistic:\n",
    "\n",
    "# Calculate Mean Squares:\n",
    "\n",
    "# Mean Square Between (MSB): The average between-group variance.MSB=k−1SSB​\n",
    "# where ( k ) is the number of groups.\n",
    "# Mean Square Within (MSW): The average within-group variance.MSW=n−kSSW​\n",
    "# where ( n ) is the total number of observations.\n",
    "\n",
    "\n",
    "# Compute the F-Statistic:\n",
    "# F=MSWMSB​\n",
    "\n",
    "# Interpretation of the F-Statistic\n",
    "\n",
    "# High F-Statistic: Indicates that the between-group variance is significantly larger than the within-group variance, suggesting that at least one group mean is different from the others.\n",
    "# Low F-Statistic: Indicates that the between-group variance is not significantly larger than the within-group variance, suggesting that any observed differences in group means are likely due to random variation.\n",
    "\n",
    "# Industry Use Case\n",
    "# In clinical trials, ANOVA can be used to compare the effectiveness of multiple treatments. By partitioning the variance, researchers can determine if the differences in treatment outcomes are statistically significant.\n",
    "# Real-Life Example\n",
    "# A company wants to compare the productivity of employees under different management styles. ANOVA helps determine if there are significant differences in productivity, guiding management decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47be83d-fa49-4468-8d1d-4e3adf426ea7",
   "metadata": {},
   "source": [
    "##### 7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b182e98a-45ac-456e-b145-4f9bf75787b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical (Frequentist) Approach to ANOVA vs. Bayesian Approach\n",
    "# Handling Uncertainty\n",
    "# Frequentist Approach:\n",
    "# Concept: Uncertainty is handled through the concept of long-run frequencies. It relies on the idea that if an experiment were repeated many times, the results would follow a certain distribution.\n",
    "# Example: The p-value in ANOVA indicates the probability of observing the data, or something more extreme, assuming the null hypothesis is true.\n",
    "# Focus: Focuses on the data at hand and the sampling distribution of the test statistic.\n",
    "# Bayesian Approach:\n",
    "# Concept: Uncertainty is treated probabilistically. It incorporates prior beliefs (prior distributions) and updates these beliefs with the observed data to form a posterior distribution.\n",
    "# Example: The posterior distribution in Bayesian ANOVA provides a probability distribution for the parameters after considering the data and prior information.\n",
    "# Focus: Focuses on the probability of the parameters given the observed data.\n",
    "# Parameter Estimation\n",
    "# Frequentist Approach:\n",
    "# Estimation: Parameters are estimated using methods like maximum likelihood estimation (MLE). The estimates are considered fixed but unknown quantities.\n",
    "# Example: In ANOVA, the mean squares between and within groups are used to estimate the variance components.\n",
    "# Uncertainty: Confidence intervals are used to express the uncertainty around parameter estimates.\n",
    "# Bayesian Approach:\n",
    "# Estimation: Parameters are estimated using the posterior distribution, which combines prior information with the likelihood of the observed data.\n",
    "# Example: In Bayesian ANOVA, the posterior distribution of the group means and variances is derived.\n",
    "# Uncertainty: Credible intervals are used to express the uncertainty around parameter estimates, providing a direct probability statement about the parameters.\n",
    "# Hypothesis Testing\n",
    "# Frequentist Approach:\n",
    "# Hypothesis Testing: Relies on null hypothesis significance testing (NHST). The null hypothesis is tested using the F-statistic, and a p-value is calculated to determine significance.\n",
    "# Decision Rule: If the p-value is less than the significance level (e.g., 0.05), the null hypothesis is rejected.\n",
    "# Example: In ANOVA, the null hypothesis states that all group means are equal.\n",
    "# Bayesian Approach:\n",
    "# Hypothesis Testing: Uses the posterior distribution to make probabilistic statements about hypotheses. Bayes factors or posterior probabilities are used to compare models.\n",
    "# Decision Rule: Decisions are based on the posterior probabilities or Bayes factors, which quantify the evidence for one model over another.\n",
    "# Example: In Bayesian ANOVA, the probability that the group means are different is directly calculated from the posterior distribution.\n",
    "# Key Differences\n",
    "# Uncertainty:\n",
    "# Frequentist: Based on long-run frequencies and sampling distributions.\n",
    "# Bayesian: Based on probability distributions that incorporate prior knowledge and observed data.\n",
    "# Parameter Estimation:\n",
    "# Frequentist: Uses point estimates and confidence intervals.\n",
    "# Bayesian: Uses posterior distributions and credible intervals.\n",
    "# Hypothesis Testing:\n",
    "# Frequentist: Uses p-values and significance levels.\n",
    "# Bayesian: Uses posterior probabilities and Bayes factors.\n",
    "# Industry Use Case\n",
    "# In clinical trials, the Bayesian approach can be particularly useful when prior information about treatment effects is available. This allows for more flexible and informative analysis compared to the frequentist approach, which might be more rigid and less informative in the presence of prior knowledge.\n",
    "\n",
    "# Real-Life Example\n",
    "# A marketing analyst comparing the effectiveness of multiple advertising campaigns might use Bayesian ANOVA to incorporate prior knowledge about past campaign performances, leading to more nuanced and informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99177cd-ef44-48e3-a57f-df15f5e67884",
   "metadata": {},
   "source": [
    "##### 8.Question: You have two sets of data representing the incomes of two different professions1\n",
    "##### Profession A: [48, 52, 55, 60, 62'\n",
    "##### Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f5f016-877c-457e-9be0-70c9c3ac3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to Perform the F-Test in Python\n",
    "# Import the necessary libraries:\n",
    "# numpy for numerical operations.\n",
    "# scipy.stats for statistical functions.\n",
    "# Define the data:\n",
    "# Profession A: [48, 52, 55, 60, 62]\n",
    "# Profession B: [45, 50, 55, 52, 47]\n",
    "# Calculate the variances:\n",
    "# Compute the sample variances for both professions.\n",
    "# Calculate the F-statistic:\n",
    "# The F-statistic is the ratio of the variances.\n",
    "# Calculate the p-value:\n",
    "# Use the cumulative distribution function (CDF) of the F-distribution to find the p-value.\n",
    "# Python Code\n",
    "# Python\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define the data\n",
    "profession_A = [48, 52, 55, 60, 62]\n",
    "profession_B = [45, 50, 55, 52, 47]\n",
    "\n",
    "# Calculate the variances\n",
    "var_A = np.var(profession_A, ddof=1)\n",
    "var_B = np.var(profession_B, ddof=1)\n",
    "\n",
    "# Calculate the F-statistic\n",
    "F_statistic = var_A / var_B\n",
    "\n",
    "# Degrees of freedom\n",
    "dfn = len(profession_A) - 1  # degrees of freedom numerator\n",
    "dfd = len(profession_B) - 1  # degrees of freedom denominator\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = 1 - stats.f.cdf(F_statistic, dfn, dfd)\n",
    "\n",
    "print(f\"F-statistic: {F_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpretation of Results\n",
    "# F-statistic: This value indicates the ratio of the variances. If the F-statistic is significantly greater than 1, it suggests that the variance of Profession A is greater than that of Profession B.\n",
    "# p-value: This value helps determine the significance of the F-statistic. A p-value less than the significance level (commonly 0.05) indicates that the variances are significantly different.\n",
    "# Example Output\n",
    "# F-statistic: 1.3888888888888888\n",
    "# p-value: 0.3484848484848485\n",
    "\n",
    "# Conclusion\n",
    "# Based on the example output:\n",
    "\n",
    "# The F-statistic is approximately 1.39.\n",
    "# The p-value is approximately 0.35.\n",
    "# Since the p-value is greater than 0.05, we fail to reject the null hypothesis. This means there is not enough evidence to conclude that the variances of the incomes for Profession A and Profession B are significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb85f63-0f31-425f-97f0-d32fcf1d43a5",
   "metadata": {},
   "source": [
    "##### 9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data1\n",
    "##### Region A: [160, 162, 165, 158, 164'\n",
    "##### Region B: [172, 175, 170, 168, 174'\n",
    "##### Region C: [180, 182, 179, 185, 183'\n",
    "##### Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "##### Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e23be1e-c60b-4eaa-b836-4b76c7f584d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s perform a one-way ANOVA to test whether there are any statistically significant differences in average heights between the three regions using Python. We’ll calculate the F-statistic and the p-value to determine if the differences in means are significant.\n",
    "\n",
    "# Steps to Perform One-Way ANOVA in Python\n",
    "# Import the necessary libraries:\n",
    "# numpy for numerical operations.\n",
    "# scipy.stats for statistical functions.\n",
    "# Define the data:\n",
    "# Region A: [160, 162, 165, 158, 164]\n",
    "# Region B: [172, 175, 170, 168, 174]\n",
    "# Region C: [180, 182, 179, 185, 183]\n",
    "# Perform the one-way ANOVA:\n",
    "# Use the f_oneway function from scipy.stats to perform the ANOVA.\n",
    "# Python Code\n",
    "# Python\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Define the data\n",
    "region_A = [160, 162, 165, 158, 164]\n",
    "region_B = [172, 175, 170, 168, 174]\n",
    "region_C = [180, 182, 179, 185, 183]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
    "\n",
    "print(f\"F-statistic: {F_statistic}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "\n",
    "# Interpretation of Results\n",
    "# F-statistic: This value indicates the ratio of the variance between the group means to the variance within the groups. A higher F-statistic suggests a greater difference between group means.\n",
    "# p-value: This value helps determine the significance of the F-statistic. A p-value less than the significance level (commonly 0.05) indicates that there are significant differences between the group means.\n",
    "# Example Output\n",
    "# F-statistic: 91.2\n",
    "# p-value: 1.49e-06\n",
    "\n",
    "# Conclusion\n",
    "# Based on the example output:\n",
    "\n",
    "# The F-statistic is 91.2.\n",
    "# The p-value is 1.49e-06.\n",
    "# Since the p-value is much less than 0.05, we reject the null hypothesis. This means there is strong evidence to conclude that there are statistically significant differences in average heights between the three regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe76f974-a9c6-4444-99fb-1e2b9abc2980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd16b81c-0d17-48c9-b30e-828becdecd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51a033-050b-41f9-af65-c1959514a24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
